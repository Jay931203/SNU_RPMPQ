{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e676fe",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# Environment Setup (with kernel caching)\n# ============================================================\nexec(open(\"/content/drive/MyDrive/MambaCompression/setup_colab.py\").read())\n!pip install compressai --quiet 2>/dev/null"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171cd0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 2: Full RP-MPQ Analysis (INT8 + Quantization)\n",
    "# ============================================================\n",
    "%cd /content/drive/MyDrive/MambaCompression/MambaIC\n",
    "\n",
    "!python train_ae.py \\\n",
    "  --checkpoint \"saved_models/mamba_transnet_L2_dim512_baseline/best.pth\" \\\n",
    "  --encoder mamba \\\n",
    "  --decoder transnet \\\n",
    "  --encoded_dim 512 \\\n",
    "  --train_path data/DATA_Htrainout.mat \\\n",
    "  --test_path data/DATA_Htestout.mat \\\n",
    "  --epochs 0 \\\n",
    "  --learning-rate 1e-3 \\\n",
    "  --decoder_layers 2 \\\n",
    "  --encoder_layers 2 \\\n",
    "  --batch-size 200 \\\n",
    "  --num-workers 4 \\\n",
    "  --quant_type asym \\\n",
    "  --pq INT8 \\\n",
    "  --aq 8 \\\n",
    "  --act_quant 16 \\\n",
    "  --analyze_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b121a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 3: FP32 Baseline Inference (No Quantization)\n",
    "# Expected NMSE: -15.34 dB (outdoor, mamba+transnet, dim=512)\n",
    "# ============================================================\n",
    "%cd /content/drive/MyDrive/MambaCompression/MambaIC\n",
    "\n",
    "!python train_ae.py \\\n",
    "  --checkpoint \"saved_models/mamba_transnet_L2_dim512_baseline/best.pth\" \\\n",
    "  --encoder mamba \\\n",
    "  --decoder transnet \\\n",
    "  --encoded_dim 512 \\\n",
    "  --train_path data/DATA_Htrainout.mat \\\n",
    "  --test_path data/DATA_Htestout.mat \\\n",
    "  --epochs 0 \\\n",
    "  --decoder_layers 2 \\\n",
    "  --encoder_layers 2 \\\n",
    "  --batch-size 200 \\\n",
    "  --test-batch-size 200 \\\n",
    "  --num-workers 4 \\\n",
    "  --pq FP32 \\\n",
    "  --aq 0 \\\n",
    "  --act_quant 32"
   ]
  },
  {
   "cell_type": "code",
   "id": "i3yncwwy8x",
   "source": "# ============================================================\n# Cell 4: Mamba-Transformer AE CR=1/4 Uniform Quantization Sweep\n# Weights: INT16 / INT8 / INT4 / INT2,  Activations: INT16 (fixed)\n# Latent: 8-bit (--aq 8, CsiNet/CLNet과 동일 조건)\n# ============================================================\n%cd /content/drive/MyDrive/MambaCompression/MambaIC\n\nBASE = (\n    \"python train_ae.py\"\n    \" --checkpoint saved_models/mamba_transnet_L2_dim512_baseline/best.pth\"\n    \" --encoder mamba --decoder transnet --encoded_dim 512\"\n    \" --train_path data/DATA_Htrainout.mat --test_path data/DATA_Htestout.mat\"\n    \" --epochs 0 --decoder_layers 2 --encoder_layers 2\"\n    \" --batch-size 200 --test-batch-size 200 --num-workers 2\"\n    \" --aq 8 --act_quant 16\"\n)\n\nfor prec in [\"INT16\", \"INT8\", \"INT4\", \"INT2\"]:\n    print(f\"\\n{'#'*60}\")\n    print(f\"# Running: W={prec}  A=INT16  Latent=8bit\")\n    print(f\"{'#'*60}\")\n    !{BASE} --pq {prec}",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "y23xqce9rrl",
   "source": "# ============================================================\n# Cell 5: RP-MPQ Offline Policy Search (ILP + KL Refinement)\n# Figure 2: ILP vs KL-refined policy evaluation\n# Range: 75% ~ 95% BOPs Saving | Step: 0.5% | KL Candidates: 10\n# Output: results/csv/mp_policy_lut_mamba_pruned.csv\n#         results/csv/fitting_raw_data_mamba.csv\n#         results/plots/exp1_pareto_accuracy_mamba.png\n#         ../figures/kl_vs_ilp.pdf (Figure 2)\n# ============================================================\n%cd /content/drive/MyDrive/MambaCompression/MambaIC\n\n# Remove existing CSVs to force fresh computation with new range\nimport os\ncsv_dir = \"results/csv\"\nfor f in [\"mp_policy_lut_mamba_pruned.csv\", \"fitting_raw_data_mamba.csv\"]:\n    p = os.path.join(csv_dir, f)\n    if os.path.exists(p):\n        os.remove(p)\n        print(f\"Removed old: {p}\")\n\n!python train_ae.py \\\n  --checkpoint \"saved_models/mamba_transnet_L2_dim512_baseline/best.pth\" \\\n  --encoder mamba \\\n  --decoder transnet \\\n  --encoded_dim 512 \\\n  --train_path data/DATA_Htrainout.mat \\\n  --test_path data/DATA_Htestout.mat \\\n  --epochs 0 \\\n  --decoder_layers 2 \\\n  --encoder_layers 2 \\\n  --batch-size 200 \\\n  --test-batch-size 200 \\\n  --num-workers 2 \\\n  --pq INT8 \\\n  --aq 8 \\\n  --act_quant 16 \\\n  --analyze_all",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}